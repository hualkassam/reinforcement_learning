{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coder Hussam Qassim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The make() function creates an environment, in this case a CartPole environment. This is a 2D simulation in\n",
    "which a cart can be accelerated left or right in order to balance a pole placed on top of it. After \n",
    "the environment is created, we must initialize it using the reset() method. This returns the first observation.\n",
    "Observations depend on the type of environment. For the CartPole environment, each observation is a 1D NumPy\n",
    "array containing four floats: these floats represent the cart’s horizontal position (0.0 = center), its \n",
    "velocity, the angle of the pole (0.0 = vertical), and its angular velocity. Finally, the render() method \n",
    "displays the environment\n",
    "'''\n",
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "print('The environment: ', env )\n",
    "\n",
    "obs = env.reset()\n",
    "print('the first observation: ', obs) \n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ask the environment what actions are possible\n",
    "print('What actions are possible: ', env.action_space) \n",
    "\n",
    "'''\n",
    "Discrete(2) means that the possible actions are integers 0 and 1, which represent accelerating left (0) or \n",
    "right (1). Other environments may have more discrete actions, or other kinds of actions (e.g.,continuous).\n",
    "Since the pole is leaning toward the right, let’s accelerate the cart toward the right:\n",
    "'''\n",
    "action = 1 # accelerate right\n",
    "obs, reward, done, info = env.step(action)\n",
    "print('The new observation: ', obs)\n",
    "print( 'Thereward: ', reward) \n",
    "print('This value will be True when the episode is over: ', done) \n",
    "print('This dictionary may provide extra debug information in other environments: ', info) \n",
    "\n",
    "'''\n",
    "The step() method executes the given action and returns four values:\n",
    "- obs: This is the new observation. The cart is now moving toward the right (obs[1]>0). The pole is still\n",
    "tilted toward the right (obs[2]>0 ), but its angular velocity is now negative (obs[3]<0 ), so it will likely\n",
    "be tilted toward the left after the next step.\n",
    "- reward:In this environment, you get a reward of 1.0 at every step, no matter what you do, so the goal is to\n",
    "keep running as long as possible.\n",
    "- done: This value will be True when the episode is over. This will happen when the pole tilts too much. After \n",
    "that, the environment must be reset before it can be used again.\n",
    "- info: This dictionary may provide extra debug information in other environments. This data should not be used\n",
    "for training (it would be cheating).\n",
    "'''\n",
    "'''\n",
    "Let’s hardcode a simple policy that accelerates left when the pole is leaning toward the left and accelerates \n",
    "right when the pole is leaning toward the right. We will run this policy to see the average rewards it gets \n",
    "over 500 episodes\n",
    "'''\n",
    "def basic_policy(obs):\n",
    "    angle = obs[2]\n",
    "    return 0 if angle < 0 else 1\n",
    "\n",
    "totals = []\n",
    "for episode in range(500):\n",
    "    episode_rewards = 0\n",
    "    obs = env.reset()\n",
    "    for step in range(1000): # 1000 steps max, we don't want to run forever\n",
    "        action = basic_policy(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        episode_rewards += reward\n",
    "        if done:\n",
    "            break\n",
    "    totals.append(episode_rewards)\n",
    "    \n",
    "# Let’s look at the result\n",
    "import numpy as np\n",
    "\n",
    "print('The result: ', np.mean(totals), np.std(totals), np.min(totals), np.max(totals)) \n",
    "\n",
    "'''\n",
    "Even with 500 tries, this policy never managed to keep the pole upright for more than 68 consecutive steps.\n",
    "Not great.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Let’s create a neural network policy. Just like the policy we hardcoded earlier, this neural network will\n",
    "take an observation as input, and it will output the action to be executed. More precisely, it will estimate\n",
    "a probability for each action, and then we will select an action randomly according to the estimated \n",
    "probabilities. In the case of the CartPole environment, there are just two possible actions (left or right),\n",
    "so we only need one output neuron. It will output the probability p of action 0 (left), and of course the \n",
    "probability of action 1 (right) will be 1 – p. For example, if it outputs 0.7, then we will pick action 0 with\n",
    "70% probability, and action 1 with 30% probability. You may wonder why we are picking a random action based on\n",
    "the probability given by the neural network, rather than just picking the action with the highest score. This\n",
    "approach lets the agent find the right balance between exploring new actions and exploiting the actions that\n",
    "are known to work well. Here’s an analogy: suppose you go to a restaurant for the first time, and all the \n",
    "dishes look equally appealing so you randomly pick one. If it turns out to be good, you can increase the \n",
    "probability to order it next time, but you shouldn’t increase that probability up to 100%, or else you will \n",
    "never try out the other dishes, some of which may be even better than the one you tried. Also note that in this\n",
    "particular environment, the past actions and observations can safely be ignored, since each observation \n",
    "contains the environment’s full state. If there were some hidden state, then you may need to consider past \n",
    "actions and observations as well. For example, if the environment only revealed the position of the cart but \n",
    "not its velocity, you would have to consider not only the current observation but also the previous observation\n",
    "in order to estimate the current velocity. Another example is when the observations are noisy; in that case, \n",
    "you generally want to use the past few observations to estimate the most likely current state. The CartPole \n",
    "problem is thus as simple as can be; the observations are noise-free and they contain the environment’s full\n",
    "state.\n",
    "'''\n",
    "# The code to build this neural network policy using TensorFlow\n",
    "'''\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import fully_connected\n",
    "\n",
    "# 1. Specify the neural network architecture\n",
    "n_inputs = 4 # == env.observation_space.shape[0]\n",
    "n_hidden = 4 # it's a simple task, we don't need more hidden neurons\n",
    "n_outputs = 1 # only outputs the probability of accelerating left\n",
    "initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "# 2. Build the neural network\n",
    "X = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "hidden = fully_connected(X, n_hidden, activation_fn=tf.nn.elu,\n",
    "                    weights_initializer=initializer)\n",
    "logits = fully_connected(hidden, n_outputs, activation_fn=None,\n",
    "                    weights_initializer=initializer)\n",
    "outputs = tf.nn.sigmoid(logits)\n",
    "\n",
    "# 3. Select a random action based on the estimated probabilities\n",
    "p_left_and_right = tf.concat(axis=1, values=[outputs, 1 - outputs])\n",
    "action = tf.multinomial(tf.log(p_left_and_right), num_samples=1)\n",
    "init = tf.global_variables_initializer()\n",
    "'''\n",
    "\n",
    "'''\n",
    "Let’s go through this code:\n",
    "1. After the imports, we define the neural network architecture. The number of inputs is the size of the \n",
    "observation space (which in the case of the CartPole is four), we just have four hidden units and no need for\n",
    "more, and we have just one output probability (the probability of going left).\n",
    "2. Next we build the neural network. In this example, it’s a vanilla Multi-Layer Perceptron, with a single \n",
    "output. Note that the output layer uses the logistic (sigmoid) activation function in order to output a \n",
    "probability from 0.0 to 1.0. If there were more than two possible actions, there would be one output neuron \n",
    "per action, and you would use the softmax activation function instead.\n",
    "3. Lastly, we call the multinomial() function to pick a random action. This function independentlysamples one\n",
    "(or more) integers, given the log probability of each integer. For example, if you call it with the array \n",
    "[np.log(0.5), np.log(0.2), np.log(0.3)] and with num_samples=5 , then it will output five integers, each of \n",
    "which will have a 50% probability of being 0, 20% of being 1, and 30% of being 2. In our case we just need one\n",
    "integer representing the action to take. Since the outputs tensor only contains the probability of going left,\n",
    "we must first concatenate 1-outputs to it to have a tensor containing the probability of both left and right\n",
    "actions. Note that if there were more than two possible actions, the neural network would have to output one\n",
    "probability per action so you would not need the concatenation step.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Actions: The Credit Assignment Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "If we knew what the best action was at each step, we could train the neural network as usual, by minimizing the\n",
    "cross entropy between the estimated probability and the target probability. It would just be regular supervised\n",
    "learning. However, in Reinforcement Learning the only guidance the agent gets is through rewards, and rewards\n",
    "are typically sparse and delayed. For example, if the agent manages to balance the pole for 100 steps, how can\n",
    "it know which of the 100 actions it took were good, and which of them were bad? All it knows is that the pole\n",
    "fell after the last action, but surely this last action is not entirely responsible. This is called the credit \n",
    "assignment problem: when the agent gets a reward, it is hard for it to know which actions should get credited\n",
    "(or blamed) for it. Think of a dog that gets rewarded hours after it behaved well; will it understand what it\n",
    "is rewarded for? To tackle this problem, a common strategy is to evaluate an action based on the sum of all the\n",
    "rewards that come after it, usually applying a discount rate r at each step. For example, if an agent decides\n",
    "to go right three times in a row and gets +10 reward after the first step, 0 after the second step, and finally\n",
    "–50 after the third step, then assuming we use a discount rate r = 0.8, the first action will have a total \n",
    "score of 10 + r × 0 + r 2 × (–50) = –22. If the discount rate is close to 0, then future rewards won’t count\n",
    "for much compared to immediate rewards. Conversely, if the discount rate is close to 1, then rewards far into\n",
    "the future will count almost as much as immediate rewards. Typical discount rates are 0.95 or 0.99. With a \n",
    "discount rate of 0.95, rewards 13 steps into the future count roughly for half as much as immediate rewards \n",
    "(since 0.95 13 ≈ 0.5), while with a discount rate of 0.99, rewards 69 steps into the future count for half as\n",
    "much as immediate rewards. In the CartPole environment, actions have fairly short-term effects, so choosing a \n",
    "discount rate of 0.95 seems reasonable. Of course, a good action may be followed by several bad actions that\n",
    "cause the pole to fall quickly, resulting in the good action getting a low score (similarly, a good actor may\n",
    "sometimes star in a terrible movie). However, if we play the game enough times, on average good actions will\n",
    "get a better score than bad ones. So, to get fairly reliable action scores, we must run many episodes and \n",
    "normalize all the action scores (by subtracting the mean and dividing by the standard deviation). After that,\n",
    "we can reasonably assume that actions with a negative score were bad while actions with a positive score were\n",
    "good.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "PG algorithms optimize the parameters of a policy by following the gradients toward higher rewards. One popular\n",
    "class of PG algorithms, called REINFORCE algorithms, was introduced back in 1992 by Ronald Williams. Here is \n",
    "one common variant:\n",
    "1. First, let the neural network policy play the game several times and at each step compute the gradients that\n",
    "would make the chosen action even more likely, but don’t apply these gradients yet.\n",
    "2. Once you have run several episodes, compute each action’s score (using the method described in the previous\n",
    "paragraph).\n",
    "3. If an action’s score is positive, it means that the action was good and you want to apply the gradients \n",
    "computed earlier to make the action even more likely to be chosen in the future. However, if the score is \n",
    "negative, it means the action was bad and you want to apply the opposite gradients to make this action slightly\n",
    "less likely in the future. The solution is simply to multiply each gradient vector by the corresponding\n",
    "action’s score.\n",
    "4. Finally, compute the mean of all the resulting gradient vectors, and use it to perform a Gradient Descent\n",
    "step.\n",
    "\n",
    "Let’s implement this algorithm using TensorFlow. We will train the neural network policy we built earlier so \n",
    "that it learns to balance the pole on the cart. Let’s start by completing the construction phase we coded \n",
    "earlier to add the target probability, the cost function, and the training operation. Since we are acting as\n",
    "though the chosen action is the best possible action, the target probability must be 1.0 if the chosen action\n",
    "is action 0 (left) and 0.0 if it is action 1 (right)\n",
    "'''\n",
    "# y = 1. - tf.to_float(action)\n",
    "\n",
    "'''\n",
    "Now that we have a target probability, we can define the cost function (cross\tentropy) and compute the\n",
    "gradients\n",
    "'''\n",
    "# learning_rate = 0.01\n",
    "# cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "#                                                labels=y, logits=logits)\n",
    "# optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "# grads_and_vars = optimizer.compute_gradients(cross_entropy)\n",
    "\n",
    "'''\n",
    "Note that we are calling the optimizer’s compute_gradients() method instead of the minimize() method. This is\n",
    "because we want to tweak the gradients before we apply them. The compute_gradients() method returns a list of\n",
    "gradient vector/variable pairs (one pair per trainable variable). Let’s put all the gradients in a list, to \n",
    "make it more convenient to obtain their values\n",
    "'''\n",
    "# gradients = [grad for grad, variable in grads_and_vars]\n",
    "\n",
    "'''\n",
    "Okay, now comes the tricky part. During the execution phase, the algorithm will run the policy and at each\n",
    "step it will evaluate these gradient tensors and store their values. After a number of episodes it will tweak\n",
    "these gradients as explained earlier (i.e., multiply them by the action scores and normalize them) and compute\n",
    "the mean of the tweaked gradients. Next, it will need to feed the resulting gradients back to the optimizer so\n",
    "that it can perform an optimization step. This means we need one placeholder per gradient vector. Moreover, we\n",
    "must create the operation that will apply the updated gradients. For this we will call the optimizer’s \n",
    "apply_gradients() function, which takes a list of gradient vector/variable pairs. Instead of giving it the \n",
    "original gradient vectors, we will give it a list containing the updated gradients (i.e., the ones fed \n",
    "through the gradient placeholders)\n",
    "'''\n",
    "# gradient_placeholders = []\n",
    "# grads_and_vars_feed = []\n",
    "# for grad, variable in grads_and_vars:\n",
    "#    gradient_placeholder = tf.placeholder(tf.float32, shape=grad.get_shape())\n",
    "#    gradient_placeholders.append(gradient_placeholder)\n",
    "#    grads_and_vars_feed.append((gradient_placeholder, variable))\n",
    "    \n",
    "#training_op = optimizer.apply_gradients(grads_and_vars_feed)\n",
    "\n",
    "# Let’s step back and take a look at the full construction phase\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import fully_connected\n",
    "\n",
    "n_inputs = 4\n",
    "n_hidden = 4\n",
    "n_outputs = 1\n",
    "initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.placeholder(tf.float32,\tshape=[None, n_inputs])\n",
    "\n",
    "hidden = fully_connected(X, n_hidden, activation_fn=tf.nn.elu,\n",
    "                    weights_initializer=initializer)\n",
    "logits = fully_connected(hidden, n_outputs,\tactivation_fn=None,\n",
    "                    weights_initializer=initializer)\n",
    "outputs = tf.nn.sigmoid(logits)\n",
    "p_left_and_right = tf.concat(axis=1, values=[outputs, 1 - outputs])\n",
    "action = tf.multinomial(tf.log(p_left_and_right), num_samples=1)\n",
    "\n",
    "y = 1. - tf.to_float(action)\n",
    "cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                                                labels=y,\tlogits=logits)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "grads_and_vars = optimizer.compute_gradients(cross_entropy)\n",
    "gradients = [grad for grad, variable in grads_and_vars]\n",
    "\n",
    "gradient_placeholders = []\n",
    "grads_and_vars_feed = []\n",
    "for grad, variable in grads_and_vars:\n",
    "    gradient_placeholder = tf.placeholder(tf.float32, shape=grad.get_shape())\n",
    "    gradient_placeholders.append(gradient_placeholder)\n",
    "    grads_and_vars_feed.append((gradient_placeholder, variable))\n",
    "training_op = optimizer.apply_gradients(grads_and_vars_feed)\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "'''\n",
    "On to the execution phase! We will need a couple of functions to compute the total discounted rewards,\n",
    "given the raw rewards, and to normalize the results across multiple episodes\n",
    "'''\n",
    "def discount_rewards(rewards, discount_rate):\n",
    "    discounted_rewards = np.empty(len(rewards))\n",
    "    cumulative_rewards = 0\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        cumulative_rewards = rewards[step] + cumulative_rewards * discount_rate\n",
    "        discounted_rewards[step] = cumulative_rewards\n",
    "    return\tdiscounted_rewards\n",
    "\n",
    "def discount_and_normalize_rewards(all_rewards, discount_rate):\n",
    "    all_discounted_rewards = [discount_rewards(rewards, discount_rate) for rewards in all_rewards]\n",
    "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
    "    reward_mean = flat_rewards.mean()\n",
    "    reward_std = flat_rewards.std()\n",
    "    return [(discounted_rewards - reward_mean)/reward_std for discounted_rewards in all_discounted_rewards]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let’s check that this works\n",
    "print('The discount_rewards: ', discount_rewards([10, 0, -50], discount_rate=0.8)) \n",
    "print('The discount_and_normalize_rewards: \\n', discount_and_normalize_rewards([[10, 0, -50], [10, 20]], \n",
    "                                                                             discount_rate=0.8))\n",
    "\n",
    "'''\n",
    "The call to discount_rewards() returns exactly what we expect. You can verify that the function \n",
    "discount_and_normalize_rewards() does indeed return the normalized scores for each action in both episodes. \n",
    "Notice that the first episode was much worse than the second, so its normalized scores are all negative; all\n",
    "actions from the first episode would be considered bad, and conversely all actions from the second episode \n",
    "would be considered good\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train the policy\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "n_iterations = 250 # number of training iterations\n",
    "n_max_steps = 1000 # max\tsteps\tper\tepisode\n",
    "n_games_per_update = 10 # train the policy every 10 episodes\n",
    "save_iterations = 10 # save the model every 10 training iterations\n",
    "discount_rate = 0.95\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for iteration in range(n_iterations):\n",
    "        print(\"\\rIteration: {}\".format(iteration), end=\"\")\n",
    "        all_rewards = [] # all sequences of raw rewards for each episode\n",
    "        all_gradients = [] # gradients saved at each step of each episode\n",
    "        for game in range(n_games_per_update):\n",
    "            current_rewards = [] # all raw rewards from the current episode\n",
    "            current_gradients = [] # all gradients from the current episode\n",
    "            obs = env.reset()\n",
    "            for step in range(n_max_steps):\n",
    "                action_val, gradients_val = sess.run(\n",
    "                                    [action, gradients],\n",
    "                                    feed_dict={X: obs.reshape(1, n_inputs)}) # one obs\n",
    "                obs, reward, done, info = env.step(action_val[0][0])\n",
    "                current_rewards.append(reward)\n",
    "                current_gradients.append(gradients_val)\n",
    "                if done:\n",
    "                    break\n",
    "            all_rewards.append(current_rewards)\n",
    "            all_gradients.append(current_gradients)\n",
    "        # At this point we have run the policy for 10 episodes, and we are\n",
    "        # ready for a policy update using the algorithm described earlier.\n",
    "        all_rewards = discount_and_normalize_rewards(all_rewards, discount_rate=discount_rate)\n",
    "        feed_dict = {}\n",
    "        for var_index, grad_placeholder in enumerate(gradient_placeholders):\n",
    "            # multiply the gradients by the action scores, and compute the mean\n",
    "            mean_gradients = np.mean(\n",
    "                            [reward * all_gradients[game_index][step][var_index]\n",
    "                        for game_index, rewards in enumerate(all_rewards)\n",
    "                     for step, reward in enumerate(rewards)],\n",
    "                     axis=0)\n",
    "            feed_dict[grad_placeholder] = mean_gradients\n",
    "        sess.run(training_op, feed_dict=feed_dict)\n",
    "        if iteration % save_iterations == 0:\n",
    "            saver.save(sess, \"./my_policy_net_pg.ckpt\")\n",
    "\n",
    "'''\n",
    "Each training iteration starts by running the policy for 10 episodes (with maximum 1,000 steps per episode, \n",
    "to avoid running forever). At each step, we also compute the gradients, pretending that the chosen action was\n",
    "the best. After these 10 episodes have been run, we compute the action scores using the discount_and_\n",
    "normalize_rewards() function; we go through each trainable variable, across all episodes and all steps, to \n",
    "multiply each gradient vector by its corresponding action score; and we compute the mean of the resulting\n",
    "gradients. Finally, we run the training operation, feeding it these mean gradients (one per trainable variable).\n",
    "We also save the model every 10 training operations. And we’re done! This code will train the neural network\n",
    "policy, and it will successfully learn to balance the pole on the cart. Note that there are actually two ways \n",
    "the agent can lose the game: either the pole can tilt too much, or the cart can go completely off the screen.\n",
    "With 250 training iterations, the policy learns to balance the pole quite well, but it is not yet good enough\n",
    "at avoiding going off the screen. A few hundred more training iterations will fix that. Despite its relative \n",
    "simplicity, this algorithm is quite powerful. You can use it to tackle much harder problems than balancing a\n",
    "pole on a cart. In fact, AlphaGo was based on a similar PG algorithm\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning to Play Ms. Pac-Man Using Deep Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Ms. Pac-Man environment\n",
    "import gym \n",
    "\n",
    "env = gym.make(\"MsPacman-v0\")\n",
    "obs = env.reset()\n",
    "print('The observation: ', obs.shape) # [height, width, channels]\n",
    "print('The environment space: ', env.action_space) \n",
    "\n",
    "'''\n",
    "As you can see, there are nine discrete actions available, which correspond to the nine possible positions\n",
    "of the joystick (left, right, up, down, center, upper left, and so on), and the observations are simply \n",
    "screenshots of the Atari screen, represented as 3D NumPy arrays. These images are a bit large, so we will \n",
    "create a small preprocessing function that will crop the image and shrink it down to 88 × 80 pixels, convert\n",
    "it to grayscale, and improve the contrast of Ms. Pac-Man. This will reduce the amount of computations required\n",
    "by the DQN, and speed up training\n",
    "'''\n",
    "mspacman_color = np.array([210, 164, 74]).mean()\n",
    "def preprocess_observation(obs):\n",
    "    img = obs[1:176:2, ::2] # crop and downsize\n",
    "    img = img.mean(axis=2) # to greyscale\n",
    "    img[img==mspacman_color] = 0 # improve\tcontrast\n",
    "    img = (img - 128) / 128 - 1 # normalize from -1. to 1.\n",
    "    return img.reshape(88, 80, 1)\n",
    "\n",
    "'''\n",
    "Next, let’s create the DQN. It could just take a state-action pair (s,a) as input, and output an estimate of\n",
    "the corresponding Q-Value Q(s,a), but since the actions are discrete it is more convenient to use a neural\n",
    "network that takes only a state s as input and outputs one Q-Value estimate per action. The DQN will be\n",
    "composed of three convolutional layers, followed by two fully connected layers, including the output layer.\n",
    "As we will see, the training algorithm we will use requires two DQNs with the same architecture (but different\n",
    "parameters): one will be used to drive Ms. Pac-Man during training (the actor), and the other will watch the\n",
    "actor and learn from its trials and errors (the critic). At regular intervals we will copy the critic to the \n",
    "actor. Since we need two identical DQNs, we will create a q_network() function to build them\n",
    "'''\n",
    "from tensorflow.contrib.layers import convolution2d, fully_connected\n",
    "\n",
    "input_height = 88\n",
    "input_width = 80\n",
    "input_channels = 1\n",
    "conv_n_maps = [32, 64, 64]\n",
    "conv_kernel_sizes = [(8,8), (4,4), (3,3)]\n",
    "conv_strides = [4, 2, 1]\n",
    "conv_paddings = [\"SAME\"]*3\n",
    "conv_activation = [tf.nn.relu]*3\n",
    "n_hidden_in = 64 * 11 * 10 # conv3 has 64 maps of 11x10 each\n",
    "n_hidden = 512\n",
    "hidden_activation = tf.nn.relu\n",
    "n_outputs = env.action_space.n # 9 discrete actions are available\n",
    "initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "def q_network(X_state,\tscope):\n",
    "    prev_layer = X_state\n",
    "    conv_layers = []\n",
    "    with tf.variable_scope(scope) as scope:\n",
    "    for n_maps, kernel_size, stride, padding, activation in zip(\n",
    "                                        conv_n_maps, conv_kernel_sizes, conv_strides,\n",
    "                                        conv_paddings,\tconv_activation):\n",
    "        prev_layer = convolution2d(\n",
    "                        prev_layer, num_outputs=n_maps, kernel_size=kernel_size,\n",
    "                        stride=stride, padding=padding, activation_fn=activation,\n",
    "                    weights_initializer=initializer)\n",
    "        conv_layers.append(prev_layer)\n",
    "        last_conv_layer_flat = tf.reshape(prev_layer, shape=[-1, n_hidden_in])\n",
    "        hidden = fully_connected(\n",
    "                        last_conv_layer_flat, n_hidden, activation_fn=hidden_activation,\n",
    "                        weights_initializer=initializer)\n",
    "        outputs = fully_connected(\n",
    "                            hidden, n_outputs, activation_fn=None,\n",
    "                            weights_initializer=initializer)\n",
    "    trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                                    scope=scope.name)\n",
    "    trainable_vars_by_name = {var.name[len(scope.name):]: var\n",
    "                                                for var in trainable_vars}\n",
    "    return outputs, trainable_vars_by_name\n",
    "\n",
    "'''\n",
    "The first part of this code defines the hyperparameters of the DQN architecture. Then the q_network() function\n",
    "creates the DQN, taking the environment’s state X_state as input, and the name of the variable scope. Note that\n",
    "we will just use one observation to represent the environment’s state since there’s almost no hidden state \n",
    "(except for blinking objects and the ghosts’ directions). The trainable_vars_by_name dictionary gathers all the\n",
    "trainable variables of this DQN. It will be useful in a minute when we create operations to copy the critic DQN\n",
    "to the actor DQN. The keys of the dictionary are the names of the variables, stripping the part of the prefix\n",
    "that just corresponds to the scope’s name. It looks like this\n",
    "'''\n",
    "print(trainable_vars_by_name)\n",
    "\n",
    "# Create the input placeholder, the two DQNs, and the operation to copy the critic DQN to the actor DQN\n",
    "X_state = tf.placeholder(tf.float32, shape=[None, input_height, input_width,\n",
    "                    input_channels])\n",
    "actor_q_values, actor_vars = q_network(X_state, scope=\"q_networks/actor\")\n",
    "critic_q_values, critic_vars = q_network(X_state, scope=\"q_networks/critic\")\n",
    "copy_ops = [actor_var.assign(critic_vars[var_name])\n",
    "                for var_name, actor_var in actor_vars.items()]\n",
    "copy_critic_to_actor = tf.group(*copy_ops)\n",
    "\n",
    "'''\n",
    "Let’s step back for a second: we now have two DQNs that are both capable of taking an environment state\n",
    "(i.e., a preprocessed observation) as input and outputting an estimated Q-Value for each possible action in\n",
    "that state. Plus we have an operation called copy_critic_to_actor to copy all the trainable variables\n",
    "of the critic DQN to the actor DQN. We use TensorFlow’s tf.group() function to group all the assignment \n",
    "operations into a single convenient operation. The actor DQN can be used to play Ms. Pac-Man (initially \n",
    "very badly). As discussed earlier, you want it to explore the game thoroughly enough, so you generally want to \n",
    "combine it with an ε-greedy policy or another exploration strategy. But what about the critic DQN? How will it\n",
    "learn to play the game? The short answer is that it will try to make its Q-Value predictions match the Q-Values\n",
    "estimated by the actor through its experience of the game. Specifically, we will let the actor play for a \n",
    "while, storing all its experiences in a replay memory. Each memory will be a 5-tuple (state, action, next \n",
    "state, reward, continue), where the “continue” item will be equal to 0.0 when the game is over, or 1.0 \n",
    "otherwise. Next, at regular intervals we will sample a batch of memories from the replay memory, and we will\n",
    "estimate the Q-Values from these memories. Finally, we will train the critic DQN to predict these Q-Values\n",
    "using regular supervised learning techniques. Once every few training iterations, we will copy the critic\n",
    "DQN to the actor DQN. \n",
    "NOTE: The replay memory is optional, but highly recommended. Without it, you would train the critic DQN using\n",
    "consecutive experiences that may be very correlated. This would introduce a lot of bias and slow down the \n",
    "training algorithm’s convergence. By using a replay memory, we ensure that the memories fed to the training\n",
    "algorithm can be fairly uncorrelated.\n",
    "Let’s add the critic DQN’s training operations. First, we need to be able to compute its predicted Q-Values \n",
    "for each state-action in the memory batch. Since the DQN outputs one Q-Value for every possible action, we \n",
    "need to keep only the Q-Value that corresponds to the action that was actually chosen in this memory. For \n",
    "this, we will convert the action to a one-hot vector (recall that this is a vector full of 0s except for a 1 \n",
    "at the i th index), and multiply it by the Q-Values: this will zero out all Q-Values except for the one \n",
    "corresponding to the memorized action. Then just sum over the first axis to obtain only the desired\n",
    "Q-Value prediction for each memory\n",
    "'''\n",
    "X_action = tf.placeholder(tf.int32, shape=[None])\n",
    "q_value = tf.reduce_sum(critic_q_values * tf.one_hot(X_action, n_outputs),\n",
    "                            axis=1,\tkeep_dims=True)\n",
    "\n",
    "'''\n",
    "Next let’s add the training operations, assuming the target Q-Values will be fed through a placeholder. We\n",
    "also create a nontrainable variable called global_step. The optimizer’s minimize() operation will take care\n",
    "of incrementing it. Plus we create the usual init operation and a Saver.\n",
    "'''\n",
    "y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "cost = tf.reduce_mean(tf.square(y - q_value))\n",
    "global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "training_op = optimizer.minimize(cost, global_step=global_step)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "'''\n",
    "That’s it for the construction phase. Before we look at the execution phase, we will need a couple of tools.\n",
    "First, let’s start by implementing the replay memory. We will use a deque list since it is very efficient at \n",
    "pushing items to the queue and popping them out from the end of the list when the maximum memory size is \n",
    "reached. We will also write a small function to\trandomly sample a batch of experiences from the replay memory\n",
    "'''\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "replay_memory_size = 10000\n",
    "replay_memory = deque([], maxlen=replay_memory_size)\n",
    "def sample_memories(batch_size):\n",
    "    indices = rnd.permutation(len(replay_memory))[:batch_size]\n",
    "    cols = [[], [], [], [], []] # state, action, reward, next_state, continue\n",
    "    for idx in indices:\n",
    "        memory = replay_memory[idx]\n",
    "        for col, value in zip(cols,\tmemory):\n",
    "            col.append(value)\n",
    "    cols = [np.array(col) for col in cols]\n",
    "    return (cols[0], cols[1], cols[2].reshape(-1, 1), cols[3],\n",
    "                cols[4].reshape(-1, 1))\n",
    "\n",
    "'''\n",
    "Next, we will need the actor to explore the game. We will use the ε-greedy policy, and gradually decrease\n",
    "ε from 1.0 to 0.05, in 50,000 training steps\n",
    "'''\n",
    "eps_min = 0.05\n",
    "eps_max = 1.0\n",
    "eps_decay_steps = 50000\n",
    "def epsilon_greedy(q_values, step):\n",
    "    epsilon = max(eps_min, eps_max - (eps_max-eps_min) * step/eps_decay_steps)\n",
    "    if rnd.rand() < epsilon:\n",
    "        return rnd.randint(n_outputs) # random action\n",
    "    else:\n",
    "        return np.argmax(q_values) # optimal action\n",
    "    \n",
    "    \n",
    "'''\n",
    "That’s it! We have all we need to start training. The execution phase does not contain anything too complex,\n",
    "but it is a bit long, so take a deep breath. Ready? Let’s go! First, let’s initialize a few variables\n",
    "'''\n",
    "\n",
    "n_steps = 100000 # total number of training steps\n",
    "training_start = 1000 # start training after 1,000 game iterations\n",
    "training_interval = 3 # run a training step every 3 game iterations\n",
    "save_steps = 50 # save the model every 50 training steps\n",
    "copy_steps = 25 # copy the critic to the actor every 25 training steps\n",
    "discount_rate = 0.95\n",
    "skip_start = 90 # skip the start of every game (it's just waiting time)\n",
    "batch_size = 50\n",
    "iteration = 0 # game iterations\n",
    "checkpoint_path = \"./my_dqn.ckpt\"\n",
    "done = True # env needs to be reset\n",
    "\n",
    "# Next, let’s open the session and run the main training loop\n",
    "with tf.Session() as sess:\n",
    "    if os.path.isfile(checkpoint_path):\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "    else:\n",
    "        init.run()\n",
    "    while True:\n",
    "        step = global_step.eval()\n",
    "        if step >= n_steps:\n",
    "            break\n",
    "        iteration += 1\n",
    "        if done: # game over, start again\n",
    "            obs = env.reset()\n",
    "            for skip in range(skip_start): # skip the start of each game\n",
    "                obs, reward, done, info = env.step(0)\n",
    "            state = preprocess_observation(obs)\n",
    "            \n",
    "        # Actor evaluates what to do\n",
    "        q_values = actor_q_values.eval(feed_dict={X_state: [state]})\n",
    "        action = epsilon_greedy(q_values, step)\n",
    "        \n",
    "        # Actor plays\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        next_state = preprocess_observation(obs)\n",
    "        \n",
    "        # Let's memorize what just happened\n",
    "        replay_memory.append((state, action, reward, next_state, 1.0 - done))\n",
    "        state = next_state\n",
    "        \n",
    "        if iteration < training_start or iteration % training_interval != 0:\n",
    "            continue\n",
    "            \n",
    "        # Critic learns\n",
    "        X_state_val, X_action_val, rewards, X_next_state_val, continues = (\n",
    "                                                            sample_memories(batch_size))\n",
    "        next_q_values = actor_q_values.eval(\n",
    "                                    feed_dict={X_state: X_next_state_val})\n",
    "        max_next_q_values = np.max(next_q_values, axis=1, keepdims=True)\n",
    "        y_val = rewards + continues * discount_rate * max_next_q_values\n",
    "        training_op.run(feed_dict={X_state: X_state_val,\n",
    "                            X_action:\tX_action_val,\ty:\ty_val})\n",
    "        \n",
    "        # Regularly copy critic to actor\n",
    "        if step % copy_steps == 0:\n",
    "            copy_critic_to_actor.run()\n",
    "        # And save regularly\n",
    "        if step % save_steps == 0:\n",
    "            saver.save(sess, checkpoint_path)\n",
    "            \n",
    "'''\n",
    "We start by restoring the models if a checkpoint file exists, or else we just initialize the variables\n",
    "normally. Then the main loop starts, where iteration counts the total number of game steps we have\n",
    "gone through since the program started, and step counts the total number of training steps since training\n",
    "started (if a checkpoint is restored, the global step is restored as well). Then the code resets the game\n",
    "(and skips the first boring game steps, where nothing happens). Next, the actor evaluates what to do, and\n",
    "plays the game, and its experience is memorized in replay memory. Then, at regular intervals (after a\n",
    "warmup period), the critic goes through a training step. It samples a batch of memories and asks the actor\n",
    "to estimate the Q-Values of all actions for the next state, and it applies Equation 16-7 to compute the target\n",
    "Q-Value y_val. The only tricky part here is that we must multiply the next state’s Q-Values by the continues\n",
    "vector to zero out the Q-Values corresponding to memories where the game was over. Next we run a training \n",
    "operation to improve the critic’s ability to predict Q-Values. Finally, at regular intervals we copy the\n",
    "critic to the actor, and we save the model.\n",
    "TIP: Unfortunately, training is very slow: if you use your laptop for training, it will take days before \n",
    "Ms. Pac-Man gets any good, and if you look at the learning curve, measuring the average rewards per episode,\n",
    "you will notice that it is extremely noisy. At some points there may be no apparent progress for a very long\n",
    "time until suddenly the agent learns to survive a reasonable amount of time. As mentioned earlier, one \n",
    "solution is to inject as much prior knowledge as possible into the model (e.g., through preprocessing, rewards,\n",
    "and so on), and you can also try to bootstrap the model by first training it to imitate a basic strategy. In\n",
    "any case, RL still requires quite a lot of patience and tweaking, but the end result is very exciting.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
